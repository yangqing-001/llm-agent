"""
æ•°æ®åº“è¡¨è®¾è®¡åˆ†æMCPæœåŠ¡å™¨

æä¾›ä¸“ä¸šçš„æ•°æ®åº“è¡¨è®¾è®¡è¯„åˆ¤åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- è¡¨ç»“æ„åˆ†æ
- å‘½åè§„èŒƒæ£€æŸ¥
- æ€§èƒ½ä¼˜åŒ–å»ºè®®
- ç´¢å¼•è®¾è®¡è¯„ä¼°
- æ•°æ®ç±»å‹åˆç†æ€§åˆ†æ
"""

from mcp.server.fastmcp import FastMCP
import logging
import pymysql
import os
from typing import Optional, Dict, List, Any
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—è®°å½•å™¨
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# åˆ›å»ºFastMCPå®ä¾‹
mcp = FastMCP("Table Design Analyzer")

# MySQLè¿æ¥é…ç½® - ä»ç¯å¢ƒå˜é‡è·å–
MYSQL_CONFIG = {
    'host': os.getenv('MYSQL_HOST', '127.0.0.1'),
    'port': int(os.getenv('MYSQL_PORT', '3306')),
    'user': os.getenv('MYSQL_USER', 'root'),
    'password': os.getenv('MYSQL_PASSWORD', '123456'),
    'charset': 'utf8mb4',
    'autocommit': True
}

def get_mysql_connection(database: Optional[str] = None) -> pymysql.Connection:
    """è·å–MySQLæ•°æ®åº“è¿æ¥"""
    config = MYSQL_CONFIG.copy()
    if database:
        config['database'] = database
    
    try:
        connection = pymysql.connect(**config)
        logger.info(f"æˆåŠŸè¿æ¥åˆ°MySQLæ•°æ®åº“: {config['host']}:{config['port']}")
        return connection
    except Exception as e:
        logger.error(f"MySQLè¿æ¥å¤±è´¥: {e}")
        raise

def get_table_detailed_info(database_name: str, table_name: str) -> Dict[str, Any]:
    """è·å–è¡¨çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬å­—æ®µã€ç´¢å¼•ã€çº¦æŸç­‰"""
    try:
        connection = get_mysql_connection(database_name)
        cursor = connection.cursor(pymysql.cursors.DictCursor)
        
        # è·å–è¡¨ç»“æ„ä¿¡æ¯
        cursor.execute(f"DESCRIBE `{table_name}`")
        columns = cursor.fetchall()
        
        # è·å–ç´¢å¼•ä¿¡æ¯
        cursor.execute(f"SHOW INDEX FROM `{table_name}`")
        indexes = cursor.fetchall()
        
        # è·å–è¡¨çŠ¶æ€ä¿¡æ¯
        cursor.execute(f"SHOW TABLE STATUS LIKE '{table_name}'")
        table_status = cursor.fetchone()
        
        # è·å–å»ºè¡¨è¯­å¥
        cursor.execute(f"SHOW CREATE TABLE `{table_name}`")
        create_table = cursor.fetchone()
        
        cursor.close()
        connection.close()
        
        return {
            'columns': columns,
            'indexes': indexes,
            'table_status': table_status,
            'create_table_sql': create_table['Create Table'] if create_table else None
        }
        
    except Exception as e:
        logger.error(f"è·å–è¡¨è¯¦ç»†ä¿¡æ¯å¤±è´¥: {e}")
        raise

def analyze_naming_conventions(table_name: str, columns: List[Dict]) -> List[str]:
    """åˆ†æå‘½åè§„èŒƒ"""
    issues = []
    
    # æ£€æŸ¥è¡¨åå‘½åè§„èŒƒ
    if not table_name.islower():
        issues.append("âŒ è¡¨åå»ºè®®ä½¿ç”¨å°å†™å­—æ¯")
    
    if ' ' in table_name:
        issues.append("âŒ è¡¨åä¸åº”åŒ…å«ç©ºæ ¼")
    
    if not table_name.replace('_', '').isalnum():
        issues.append("âŒ è¡¨ååº”åªåŒ…å«å­—æ¯ã€æ•°å­—å’Œä¸‹åˆ’çº¿")
    
    # æ£€æŸ¥å­—æ®µå‘½åè§„èŒƒ
    for col in columns:
        field_name = col['Field']
        
        if not field_name.islower():
            issues.append(f"âŒ å­—æ®µ '{field_name}' å»ºè®®ä½¿ç”¨å°å†™å­—æ¯")
        
        if ' ' in field_name:
            issues.append(f"âŒ å­—æ®µ '{field_name}' ä¸åº”åŒ…å«ç©ºæ ¼")
        
        # æ£€æŸ¥ä¿ç•™å­—
        mysql_reserved_words = ['order', 'group', 'select', 'from', 'where', 'insert', 'update', 'delete']
        if field_name.lower() in mysql_reserved_words:
            issues.append(f"âš ï¸ å­—æ®µ '{field_name}' æ˜¯MySQLä¿ç•™å­—ï¼Œå»ºè®®ä½¿ç”¨åå¼•å·æˆ–é‡å‘½å")
    
    return issues

def analyze_data_types(columns: List[Dict]) -> List[str]:
    """åˆ†ææ•°æ®ç±»å‹é€‰æ‹©çš„åˆç†æ€§"""
    suggestions = []
    
    for col in columns:
        field_name = col['Field']
        data_type = col['Type'].lower()
        
        # æ£€æŸ¥TEXTç±»å‹ä½¿ç”¨
        if 'text' in data_type and col['Key'] != '':
            suggestions.append(f"âš ï¸ å­—æ®µ '{field_name}' ä½¿ç”¨TEXTç±»å‹ä¸”æœ‰ç´¢å¼•ï¼Œå¯èƒ½å½±å“æ€§èƒ½")
        
        # æ£€æŸ¥VARCHARé•¿åº¦
        if 'varchar' in data_type:
            # æå–é•¿åº¦
            import re
            length_match = re.search(r'varchar\((\d+)\)', data_type)
            if length_match:
                length = int(length_match.group(1))
                if length > 255:
                    suggestions.append(f"ğŸ’¡ å­—æ®µ '{field_name}' VARCHARé•¿åº¦ä¸º{length}ï¼Œè€ƒè™‘æ˜¯å¦éœ€è¦ä½¿ç”¨TEXTç±»å‹")
                elif length < 10 and 'id' not in field_name.lower():
                    suggestions.append(f"ğŸ’¡ å­—æ®µ '{field_name}' VARCHARé•¿åº¦è¾ƒçŸ­({length})ï¼Œè€ƒè™‘æ˜¯å¦åˆé€‚")
        
        # æ£€æŸ¥INTç±»å‹
        if data_type.startswith('int') and 'id' in field_name.lower():
            if 'auto_increment' not in col['Extra'].lower():
                suggestions.append(f"ğŸ’¡ å­—æ®µ '{field_name}' çœ‹èµ·æ¥æ˜¯IDå­—æ®µï¼Œè€ƒè™‘æ·»åŠ AUTO_INCREMENT")
        
        # æ£€æŸ¥æ—¶é—´å­—æ®µ
        if any(time_word in field_name.lower() for time_word in ['time', 'date', 'created', 'updated']):
            if not any(time_type in data_type for time_type in ['datetime', 'timestamp', 'date', 'time']):
                suggestions.append(f"ğŸ’¡ å­—æ®µ '{field_name}' çœ‹èµ·æ¥æ˜¯æ—¶é—´å­—æ®µï¼Œå»ºè®®ä½¿ç”¨DATETIMEæˆ–TIMESTAMPç±»å‹")
    
    return suggestions

def analyze_indexes(indexes: List[Dict], columns: List[Dict]) -> List[str]:
    """åˆ†æç´¢å¼•è®¾è®¡"""
    suggestions = []
    
    # ç»Ÿè®¡ç´¢å¼•ä¿¡æ¯
    index_info = {}
    for idx in indexes:
        key_name = idx['Key_name']
        if key_name not in index_info:
            index_info[key_name] = {
                'columns': [],
                'unique': idx['Non_unique'] == 0,
                'type': idx['Index_type']
            }
        index_info[key_name]['columns'].append(idx['Column_name'])
    
    # æ£€æŸ¥ä¸»é”®
    if 'PRIMARY' not in index_info:
        suggestions.append("âŒ è¡¨ç¼ºå°‘ä¸»é”®ï¼Œå¼ºçƒˆå»ºè®®æ·»åŠ ä¸»é”®")
    
    # æ£€æŸ¥è¿‡å¤šç´¢å¼•
    non_primary_indexes = [k for k in index_info.keys() if k != 'PRIMARY']
    if len(non_primary_indexes) > 5:
        suggestions.append(f"âš ï¸ è¡¨æœ‰{len(non_primary_indexes)}ä¸ªéä¸»é”®ç´¢å¼•ï¼Œè¿‡å¤šç´¢å¼•å¯èƒ½å½±å“å†™å…¥æ€§èƒ½")
    
    # æ£€æŸ¥é‡å¤ç´¢å¼•
    column_sets = []
    for idx_name, idx_info in index_info.items():
        if idx_name != 'PRIMARY':
            col_set = tuple(sorted(idx_info['columns']))
            if col_set in column_sets:
                suggestions.append(f"âš ï¸ å¯èƒ½å­˜åœ¨é‡å¤ç´¢å¼•ï¼Œè¯·æ£€æŸ¥ç´¢å¼• '{idx_name}'")
            column_sets.append(col_set)
    
    # æ£€æŸ¥å¤–é”®å­—æ®µæ˜¯å¦æœ‰ç´¢å¼•
    for col in columns:
        field_name = col['Field']
        if field_name.endswith('_id') and field_name != 'id':
            # æ£€æŸ¥æ˜¯å¦æœ‰ç´¢å¼•
            has_index = any(field_name in idx_info['columns'] for idx_info in index_info.values())
            if not has_index:
                suggestions.append(f"ğŸ’¡ å¤–é”®å­—æ®µ '{field_name}' å»ºè®®æ·»åŠ ç´¢å¼•ä»¥æé«˜æŸ¥è¯¢æ€§èƒ½")
    
    return suggestions

@mcp.tool()
def analyze_table_design(database_name: str, table_name: str) -> str:
    """
    åˆ†ææ•°æ®åº“è¡¨è®¾è®¡ï¼Œæä¾›ä¸“ä¸šçš„è¯„åˆ¤å’Œä¼˜åŒ–å»ºè®®ï¼Œå¯ä»¥åˆ†æç”¨æˆ·ä¼ å…¥çš„è¡¨ç»“æ„è®¾è®¡å¦‚ä½•
    
    Args:
        database_name: æ•°æ®åº“åç§°
        table_name: è¡¨åç§°
        
    Returns:
        str: è¯¦ç»†çš„è¡¨è®¾è®¡åˆ†ææŠ¥å‘Š
    """
    try:
        # è·å–è¡¨çš„è¯¦ç»†ä¿¡æ¯
        table_info = get_table_detailed_info(database_name, table_name)
        
        columns = table_info['columns']
        indexes = table_info['indexes']
        table_status = table_info['table_status']
        
        if not columns:
            return f"âŒ è¡¨ {table_name} ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®"
        
        # å¼€å§‹åˆ†æ
        analysis_result = []
        analysis_result.append(f"ğŸ” è¡¨è®¾è®¡åˆ†ææŠ¥å‘Š: {database_name}.{table_name}")
        analysis_result.append("=" * 60)
        
        # åŸºæœ¬ä¿¡æ¯
        analysis_result.append(f"\nğŸ“Š åŸºæœ¬ä¿¡æ¯:")
        analysis_result.append(f"  â€¢ å­—æ®µæ•°é‡: {len(columns)}")
        analysis_result.append(f"  â€¢ ç´¢å¼•æ•°é‡: {len(set(idx['Key_name'] for idx in indexes))}")
        if table_status:
            analysis_result.append(f"  â€¢ å­˜å‚¨å¼•æ“: {table_status.get('Engine', 'Unknown')}")
            analysis_result.append(f"  â€¢ å­—ç¬¦é›†: {table_status.get('Collation', 'Unknown')}")
        
        # å‘½åè§„èŒƒåˆ†æ
        naming_issues = analyze_naming_conventions(table_name, columns)
        analysis_result.append(f"\nğŸ“ å‘½åè§„èŒƒæ£€æŸ¥:")
        if naming_issues:
            for issue in naming_issues:
                analysis_result.append(f"  {issue}")
        else:
            analysis_result.append("  âœ… å‘½åè§„èŒƒè‰¯å¥½")
        
        # æ•°æ®ç±»å‹åˆ†æ
        datatype_suggestions = analyze_data_types(columns)
        analysis_result.append(f"\nğŸ”§ æ•°æ®ç±»å‹åˆ†æ:")
        if datatype_suggestions:
            for suggestion in datatype_suggestions:
                analysis_result.append(f"  {suggestion}")
        else:
            analysis_result.append("  âœ… æ•°æ®ç±»å‹é€‰æ‹©åˆç†")
        
        # ç´¢å¼•è®¾è®¡åˆ†æ
        index_suggestions = analyze_indexes(indexes, columns)
        analysis_result.append(f"\nğŸš€ ç´¢å¼•è®¾è®¡åˆ†æ:")
        if index_suggestions:
            for suggestion in index_suggestions:
                analysis_result.append(f"  {suggestion}")
        else:
            analysis_result.append("  âœ… ç´¢å¼•è®¾è®¡è‰¯å¥½")
        
        # æ€»ä½“è¯„åˆ†å’Œå»ºè®®
        total_issues = len(naming_issues) + len(datatype_suggestions) + len(index_suggestions)
        analysis_result.append(f"\nğŸ“ˆ æ€»ä½“è¯„ä¼°:")
        
        if total_issues == 0:
            analysis_result.append("  ğŸŒŸ ä¼˜ç§€! è¡¨è®¾è®¡éå¸¸è§„èŒƒï¼Œæ²¡æœ‰å‘ç°æ˜æ˜¾é—®é¢˜")
        elif total_issues <= 3:
            analysis_result.append("  ğŸ‘ è‰¯å¥½! è¡¨è®¾è®¡åŸºæœ¬åˆç†ï¼Œæœ‰å°‘é‡ä¼˜åŒ–ç©ºé—´")
        elif total_issues <= 6:
            analysis_result.append("  âš ï¸ ä¸€èˆ¬! è¡¨è®¾è®¡å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œå»ºè®®ä¼˜åŒ–")
        else:
            analysis_result.append("  âŒ éœ€è¦æ”¹è¿›! è¡¨è®¾è®¡å­˜åœ¨è¾ƒå¤šé—®é¢˜ï¼Œå¼ºçƒˆå»ºè®®é‡æ„")
        
        analysis_result.append(f"  â€¢ å‘ç°é—®é¢˜/å»ºè®®æ•°é‡: {total_issues}")
        
        logger.info(f"æˆåŠŸåˆ†æè¡¨è®¾è®¡: {database_name}.{table_name}")
        return "\n".join(analysis_result)
        
    except Exception as e:
        logger.error(f"è¡¨è®¾è®¡åˆ†æå¤±è´¥: {e}")
        return f"âŒ è¡¨è®¾è®¡åˆ†æå¤±è´¥: {str(e)}"

@mcp.tool()
def get_table_structure_info(database_name: str, table_name: str) -> str:
    """
    è·å–è¡¨çš„è¯¦ç»†ç»“æ„ä¿¡æ¯ï¼ŒåŒ…æ‹¬å­—æ®µã€ç´¢å¼•ã€çº¦æŸç­‰

    Args:
        database_name: æ•°æ®åº“åç§°
        table_name: è¡¨åç§°

    Returns:
        str: è¡¨ç»“æ„çš„è¯¦ç»†ä¿¡æ¯
    """
    try:
        table_info = get_table_detailed_info(database_name, table_name)

        columns = table_info['columns']
        indexes = table_info['indexes']
        table_status = table_info['table_status']
        create_sql = table_info['create_table_sql']

        if not columns:
            return f"âŒ è¡¨ {table_name} ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®"

        result = []
        result.append(f"ğŸ“‹ è¡¨ç»“æ„ä¿¡æ¯: {database_name}.{table_name}")
        result.append("=" * 50)

        # å­—æ®µä¿¡æ¯
        result.append(f"\nğŸ”§ å­—æ®µä¿¡æ¯ ({len(columns)} ä¸ªå­—æ®µ):")
        for col in columns:
            null_info = "NULL" if col['Null'] == 'YES' else "NOT NULL"
            key_info = f" [{col['Key']}]" if col['Key'] else ""
            default_info = f" DEFAULT: {col['Default']}" if col['Default'] is not None else ""
            extra_info = f" {col['Extra']}" if col['Extra'] else ""

            result.append(f"  â€¢ {col['Field']}: {col['Type']} {null_info}{key_info}{default_info}{extra_info}")

        # ç´¢å¼•ä¿¡æ¯
        index_info = {}
        for idx in indexes:
            key_name = idx['Key_name']
            if key_name not in index_info:
                index_info[key_name] = {
                    'columns': [],
                    'unique': idx['Non_unique'] == 0,
                    'type': idx['Index_type']
                }
            index_info[key_name]['columns'].append(idx['Column_name'])

        result.append(f"\nğŸš€ ç´¢å¼•ä¿¡æ¯ ({len(index_info)} ä¸ªç´¢å¼•):")
        for idx_name, idx_details in index_info.items():
            unique_info = "UNIQUE" if idx_details['unique'] else "NON-UNIQUE"
            columns_str = ", ".join(idx_details['columns'])
            result.append(f"  â€¢ {idx_name}: ({columns_str}) - {unique_info} {idx_details['type']}")

        # è¡¨çŠ¶æ€ä¿¡æ¯
        if table_status:
            result.append(f"\nğŸ“Š è¡¨çŠ¶æ€ä¿¡æ¯:")
            result.append(f"  â€¢ å­˜å‚¨å¼•æ“: {table_status.get('Engine', 'Unknown')}")
            result.append(f"  â€¢ å­—ç¬¦é›†: {table_status.get('Collation', 'Unknown')}")
            result.append(f"  â€¢ è¡Œæ•°: {table_status.get('Rows', 'Unknown')}")
            result.append(f"  â€¢ æ•°æ®é•¿åº¦: {table_status.get('Data_length', 'Unknown')} bytes")
            result.append(f"  â€¢ ç´¢å¼•é•¿åº¦: {table_status.get('Index_length', 'Unknown')} bytes")

        # å»ºè¡¨è¯­å¥
        if create_sql:
            result.append(f"\nğŸ“ å»ºè¡¨è¯­å¥:")
            result.append(f"```sql\n{create_sql}\n```")

        logger.info(f"æˆåŠŸè·å–è¡¨ç»“æ„ä¿¡æ¯: {database_name}.{table_name}")
        return "\n".join(result)

    except Exception as e:
        logger.error(f"è·å–è¡¨ç»“æ„ä¿¡æ¯å¤±è´¥: {e}")
        return f"âŒ è·å–è¡¨ç»“æ„ä¿¡æ¯å¤±è´¥: {str(e)}"

@mcp.tool()
def check_table_performance_issues(database_name: str, table_name: str) -> str:
    """
    æ£€æŸ¥è¡¨çš„æ€§èƒ½é—®é¢˜å¹¶æä¾›ä¼˜åŒ–å»ºè®®

    Args:
        database_name: æ•°æ®åº“åç§°
        table_name: è¡¨åç§°

    Returns:
        str: æ€§èƒ½é—®é¢˜åˆ†æå’Œä¼˜åŒ–å»ºè®®
    """
    try:
        table_info = get_table_detailed_info(database_name, table_name)

        columns = table_info['columns']
        indexes = table_info['indexes']
        table_status = table_info['table_status']

        if not columns:
            return f"âŒ è¡¨ {table_name} ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®"

        performance_issues = []
        performance_issues.append(f"âš¡ æ€§èƒ½åˆ†ææŠ¥å‘Š: {database_name}.{table_name}")
        performance_issues.append("=" * 50)

        issues_found = []

        # æ£€æŸ¥ä¸»é”®
        has_primary_key = any(idx['Key_name'] == 'PRIMARY' for idx in indexes)
        if not has_primary_key:
            issues_found.append("âŒ ç¼ºå°‘ä¸»é”® - è¿™ä¼šä¸¥é‡å½±å“å¤åˆ¶å’Œæ€§èƒ½")

        # æ£€æŸ¥è¿‡é•¿çš„VARCHARå­—æ®µ
        for col in columns:
            if 'varchar' in col['Type'].lower():
                import re
                length_match = re.search(r'varchar\((\d+)\)', col['Type'].lower())
                if length_match and int(length_match.group(1)) > 500:
                    issues_found.append(f"âš ï¸ å­—æ®µ '{col['Field']}' VARCHARé•¿åº¦è¿‡é•¿ï¼Œå¯èƒ½å½±å“å†…å­˜ä½¿ç”¨")

        # æ£€æŸ¥TEXT/BLOBå­—æ®µçš„ç´¢å¼•
        for col in columns:
            if any(t in col['Type'].lower() for t in ['text', 'blob']):
                # æ£€æŸ¥æ˜¯å¦æœ‰ç´¢å¼•
                has_index = any(col['Field'] in idx['Column_name'] for idx in indexes)
                if has_index:
                    issues_found.append(f"âš ï¸ TEXT/BLOBå­—æ®µ '{col['Field']}' æœ‰ç´¢å¼•ï¼Œå¯èƒ½å½±å“æ€§èƒ½")

        # æ£€æŸ¥ç´¢å¼•æ•°é‡
        unique_indexes = set(idx['Key_name'] for idx in indexes)
        if len(unique_indexes) > 6:
            issues_found.append(f"âš ï¸ ç´¢å¼•è¿‡å¤š ({len(unique_indexes)}ä¸ª)ï¼Œå¯èƒ½å½±å“å†™å…¥æ€§èƒ½")

        # æ£€æŸ¥è¡¨å¤§å°
        if table_status:
            data_length = table_status.get('Data_length', 0)
            index_length = table_status.get('Index_length', 0)

            if isinstance(data_length, (int, float)) and data_length > 100 * 1024 * 1024:  # 100MB
                issues_found.append(f"ğŸ“Š è¡¨æ•°æ®è¾ƒå¤§ ({data_length / 1024 / 1024:.1f}MB)ï¼Œè€ƒè™‘åˆ†åŒºæˆ–å½’æ¡£")

            if isinstance(index_length, (int, float)) and isinstance(data_length, (int, float)) and data_length > 0:
                index_ratio = index_length / data_length
                if index_ratio > 0.5:
                    issues_found.append(f"ğŸ“Š ç´¢å¼•å¤§å°å æ•°æ®å¤§å°çš„ {index_ratio:.1%}ï¼Œå¯èƒ½è¿‡å¤š")

        # è¾“å‡ºç»“æœ
        if issues_found:
            performance_issues.append(f"\nğŸ” å‘ç°çš„æ€§èƒ½é—®é¢˜:")
            for issue in issues_found:
                performance_issues.append(f"  {issue}")

            performance_issues.append(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
            performance_issues.append("  â€¢ ç¡®ä¿æ¯ä¸ªè¡¨éƒ½æœ‰ä¸»é”®")
            performance_issues.append("  â€¢ é¿å…åœ¨TEXT/BLOBå­—æ®µä¸Šåˆ›å»ºç´¢å¼•")
            performance_issues.append("  â€¢ åˆç†æ§åˆ¶VARCHARå­—æ®µé•¿åº¦")
            performance_issues.append("  â€¢ å®šæœŸåˆ†æè¡¨ä½¿ç”¨æƒ…å†µï¼Œåˆ é™¤ä¸å¿…è¦çš„ç´¢å¼•")
            performance_issues.append("  â€¢ å¯¹äºå¤§è¡¨è€ƒè™‘åˆ†åŒºç­–ç•¥")
        else:
            performance_issues.append(f"\nâœ… æœªå‘ç°æ˜æ˜¾çš„æ€§èƒ½é—®é¢˜")

        logger.info(f"æˆåŠŸåˆ†æè¡¨æ€§èƒ½: {database_name}.{table_name}")
        return "\n".join(performance_issues)

    except Exception as e:
        logger.error(f"æ€§èƒ½åˆ†æå¤±è´¥: {e}")
        return f"âŒ æ€§èƒ½åˆ†æå¤±è´¥: {str(e)}"

if __name__ == "__main__":
    # å¯åŠ¨MCPæœåŠ¡å™¨
    mcp.run()
