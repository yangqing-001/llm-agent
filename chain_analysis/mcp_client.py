import asyncio
import sys
from langchain.memory import ConversationBufferWindowMemory
from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.prebuilt import create_react_agent
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from pydantic import SecretStr

# åˆå§‹åŒ– ChatOpenAI å¤§æ¨¡å‹å®¢æˆ·ç«¯ 
# llm = ChatOpenAI(
#     model="ep-20*********02-887jj",  # ç«å±±å¼•æ“æ¨ç†æ¥å…¥ç‚¹
#     api_key=SecretStr("b6a98d76-**********-b490f6ba59e3"),  # ç«å±±å¼•æ“APIå¯†é’¥
#     base_url="https://ark.cn-beijing.volces.com/api/v3",  # ç«å±±å¼•æ“APIåŸºç¡€åœ°å€
#     temperature=0.3  # é™ä½æ¸©åº¦å€¼ï¼Œå‡å°‘éšæœºæ€§
# )
llm = ChatOpenAI(
    model="qwen-max",  # æŒ‡å®šåƒé—®æ¨¡å‹
    temperature=0,
    api_key="sk-7b957493cc324362b63a95bcd09ef189",  # ç¡®ä¿ç¯å¢ƒå˜é‡æ­£ç¡®
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"  # å…¼å®¹æ¨¡å¼ç«¯ç‚¹
)

# åˆå§‹åŒ–LLM
# llm = ChatOllama(
#     model="qwen3:14b",
#     base_url="http://localhost:11434",
#     temperature=0.3
# )

# åˆ›å»ºä¼˜åŒ–çš„æç¤ºè¯
def create_smart_prompt():
    """åˆ›å»ºæ™ºèƒ½æç¤ºè¯ï¼Œå¼•å¯¼AIåˆç†ä½¿ç”¨å·¥å…·"""
    system_template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ™ºèƒ½åŠ©æ‰‹ï¼Œå…·å¤‡å¤šç§å·¥å…·èƒ½åŠ›ï¼Œèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·å®Œæˆå„ç±»ä»»åŠ¡ã€‚

å¯ç”¨å·¥å…·åŠä½¿ç”¨åœºæ™¯ï¼š
- terminal: æ‰§è¡Œç³»ç»Ÿå‘½ä»¤ã€æ–‡ä»¶æ“ä½œã€ç¨‹åºè¿è¡Œ
- app_analysis: èŒä¸šåˆ†æã€åº”ç”¨ç¨‹åºåˆ†æã€ç”¨æˆ·ç”»åƒ  
- browser: ç½‘é¡µæ§åˆ¶ã€è‡ªåŠ¨åŒ–æ“ä½œã€ä¿¡æ¯é‡‡é›†
- weather: å¤©æ°”æŸ¥è¯¢ã€æ°”è±¡ä¿¡æ¯è·å–

å·¥å…·ä½¿ç”¨åŸåˆ™ï¼š
1. åªæœ‰ç”¨æˆ·æ˜ç¡®éœ€è¦æ‰§è¡Œå…·ä½“æ“ä½œæ—¶æ‰ä½¿ç”¨å·¥å…·
2. å¯¹äºä¸€èˆ¬æ€§å¯¹è¯ã€æ¦‚å¿µè§£é‡Šã€å»ºè®®å’¨è¯¢ç­‰ï¼Œç›´æ¥å›ç­”
3. ä¼˜å…ˆé€‰æ‹©æœ€åˆé€‚çš„å·¥å…·æ¥å®Œæˆä»»åŠ¡
4. å¦‚æœä¸ç¡®å®šæ˜¯å¦éœ€è¦å·¥å…·ï¼Œä¼˜å…ˆé€‰æ‹©ç›´æ¥å›ç­”

è¯·ä¿æŒè‡ªç„¶ã€å‹å¥½çš„å¯¹è¯é£æ ¼ï¼Œå‡†ç¡®ç†è§£ç”¨æˆ·çœŸå®æ„å›¾ï¼Œæ˜æ™ºåœ°é€‰æ‹©æ˜¯å¦ä½¿ç”¨å·¥å…·ã€‚"""

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_template),
        MessagesPlaceholder(variable_name="messages")
    ])
    
    return prompt

# ç®€åŒ–çš„ç»“æœè¾“å‡ºå‡½æ•°
def print_result(agent_response):
    """è¾“å‡ºä»£ç†å“åº”ç»“æœ"""
    messages = agent_response.get("messages", [])
    
    # æå–æœ€ç»ˆç­”æ¡ˆ
    for message in reversed(messages):
        if message.type == "ai" and message.content:
            print(f"\nğŸ¯ å›ç­”: {message.content}")
            break

# ä¸»å‡½æ•°
async def main():
    """
    ä¸»å‡½æ•° - é›†æˆLangChainè®°å¿†åŠŸèƒ½çš„å¯¹è¯å®¢æˆ·ç«¯
    
    Author: FangGL
    Date: 2024-12-19
    """
    client = None
    
    try:
        # åˆå§‹åŒ–MCPå®¢æˆ·ç«¯ - å®Œæ•´æœåŠ¡å™¨é…ç½®  
        client = MultiServerMCPClient({  # type: ignore
            "terminal": {
                "command": "python",
                "args": ["./mcp_servers/terminal_server.py"],
                "transport": "stdio",
            },
            "app_analysis": {
                "command": "python",
                "args": ["./mcp_servers/application_analysis_server.py"],
                "transport": "stdio",
            },
            "browser": {
                "command": "python",
                "args": ["./mcp_servers/browser_control_server.py"],
                "transport": "stdio",
            },
            "weather": {
                "command": "python",
                "args": ["./mcp_servers/weather_server.py"],
                "transport": "stdio",
            }
        })

        tools = await client.get_tools()
        
        # åˆ›å»ºå¸¦æœ‰æ™ºèƒ½æç¤ºè¯çš„ä»£ç†
        smart_prompt = create_smart_prompt()
        agent = create_react_agent(llm, tools, prompt=smart_prompt)

        # ä½¿ç”¨çª—å£è®°å¿† - ä¿ç•™æœ€è¿‘10è½®å¯¹è¯ï¼Œæ— éœ€transformersåŒ…
        memory = ConversationBufferWindowMemory(
            k=10,                    # ä¿ç•™æœ€è¿‘10è½®å¯¹è¯  
            return_messages=True     # è¿”å›æ¶ˆæ¯å¯¹è±¡
        )

        print("âœ¨ å…¨åŠŸèƒ½æ™ºèƒ½åŠ©æ‰‹å·²å¯åŠ¨!")
        print("ğŸ”§ å®Œæ•´å·¥å…·: ç³»ç»Ÿå‘½ä»¤ | èŒä¸šåˆ†æ | æµè§ˆå™¨æ§åˆ¶ | å¤©æ°”æŸ¥è¯¢")
        print("ğŸ§  AIä¼šæ™ºèƒ½åˆ¤æ–­ä½•æ—¶éœ€è¦è°ƒç”¨å·¥å…·")
        print("ğŸ’¬ æ”¯æŒæµå¼å¯¹è¯ï¼Œä½“éªŒæ›´è‡ªç„¶")
        print("è¾“å…¥ 'exit' é€€å‡º\n")

        # å¯¹è¯å¾ªç¯
        while True:
            user_input = input("ğŸ’¬ é—®é¢˜: ").strip()
            
            if user_input.lower() == "exit":
                print("ğŸ‘‹ å†è§!")
                break
            
            if not user_input:
                continue
            
            # è·å–å†å²æ¶ˆæ¯å¹¶æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
            messages = memory.chat_memory.messages.copy()
            messages.append(HumanMessage(content=user_input))
            
            # å…ˆç”¨LLMåˆ¤æ–­æ˜¯å¦éœ€è¦å·¥å…·
            print("ğŸ¤” åˆ†æé—®é¢˜ä¸­...")
            
            # åˆ›å»ºåˆ¤æ–­æç¤º
            judge_prompt = f"""
            ç”¨æˆ·é—®é¢˜: {user_input}
            
            è¯·åˆ¤æ–­è¿™ä¸ªé—®é¢˜æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·æ¥å®Œæˆä»»åŠ¡ã€‚
            
            éœ€è¦è°ƒç”¨å·¥å…·çš„æƒ…å†µ(å›ç­”YES)ï¼š
            - æ‰§è¡Œç³»ç»Ÿå‘½ä»¤ã€æ–‡ä»¶æ“ä½œã€ç¨‹åºè¿è¡Œ
            - èŒä¸šåˆ†æã€åº”ç”¨ç¨‹åºåˆ†æã€äº†è§£èŒä¸šåŒ¹é…åº¦
            - ç½‘é¡µæ§åˆ¶ã€ä¿¡æ¯é‡‡é›†ã€æ‰“å¼€æµè§ˆå™¨
            - å¤©æ°”æŸ¥è¯¢ã€æ°”è±¡ä¿¡æ¯ã€å¤©æ°”é¢„æŠ¥
            - å…¶ä»–éœ€è¦å®é™…æ‰§è¡Œæ“ä½œçš„ä»»åŠ¡
            
            ä¸éœ€è¦å·¥å…·çš„æƒ…å†µ(å›ç­”NO)ï¼š
            - ä¸€èˆ¬æ€§å¯¹è¯ã€é—®å€™ã€é—²èŠ
            - æ¦‚å¿µè§£é‡Šã€çŸ¥è¯†é—®ç­”
            - å»ºè®®å’¨è¯¢ã€æ„è§äº¤æµ
            - åˆ›æ„å†™ä½œã€æ•…äº‹åˆ›ä½œ
            - ç®€å•æ•°å­¦è®¡ç®—
            
            åªå›ç­”YESæˆ–NOï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
            """
            
            judge_response = await llm.ainvoke([HumanMessage(content=judge_prompt)])
            need_tools = "YES" in str(judge_response.content).upper()
            
            if need_tools:
                print("ğŸ”§ æ£€æµ‹åˆ°éœ€è¦å·¥å…·è°ƒç”¨ï¼Œå¯åŠ¨å¢å¼ºæ¨¡å¼...")
                ai_response = ""
                tool_used = False
                
                # ä½¿ç”¨agentå¤„ç†å·¥å…·è°ƒç”¨
                async for chunk in agent.astream({"messages": messages}):
                    for node_name, node_output in chunk.items():
                        if node_name == "agent":
                            messages_in_chunk = node_output.get("messages", [])
                            for msg in messages_in_chunk:
                                if msg.type == "ai":
                                    if hasattr(msg, "tool_calls") and msg.tool_calls:
                                        tool_used = True
                                        for tool_call in msg.tool_calls:
                                            print(f"ğŸ”§ è°ƒç”¨å·¥å…·: {tool_call['name']}")
                                            print(f"ğŸ“ å‚æ•°: {tool_call['args']}")
                                    elif msg.content:
                                        ai_response = msg.content
                        
                        elif node_name == "tools":
                            messages_in_chunk = node_output.get("messages", [])
                            for msg in messages_in_chunk:
                                if msg.type == "tool":
                                    print(f"âœ… å·¥å…· '{msg.name}' æ‰§è¡Œå®Œæˆ")
                                    try:
                                        import json
                                        result = json.loads(msg.content) if isinstance(msg.content, str) else msg.content
                                        if isinstance(result, dict):
                                            if result.get("success"):
                                                stdout = result.get("stdout", "").strip()
                                                if stdout:
                                                    print(f"ğŸ“„ å‘½ä»¤è¾“å‡º:\n{stdout}")
                                            else:
                                                stderr = result.get("stderr", "")
                                                print(f"âŒ é”™è¯¯: {stderr}")
                                    except:
                                        print(f"ğŸ“„ åŸå§‹ç»“æœ: {msg.content}")
                
                if ai_response:
                    print(f"\nğŸ¯ æœ€ç»ˆå›ç­”: {ai_response}")
                    
            else:
                print("ğŸ’­ ä½¿ç”¨æµå¼å›ç­”æ¨¡å¼...")
                print("\nğŸ¯ AIå›ç­”: ", end="", flush=True)
                
                # ç›´æ¥ä½¿ç”¨LLMæµå¼ç”Ÿæˆ
                ai_response = ""
                async for chunk in llm.astream(messages):
                    if chunk.content:
                        content_str = str(chunk.content)
                        print(content_str, end="", flush=True)
                        ai_response += content_str
                
                print()  # æ¢è¡Œ
            
            # ä¿å­˜å¯¹è¯åˆ°è®°å¿†
            if ai_response:
                memory.save_context(
                    {"input": user_input},
                    {"output": str(ai_response)}
                )
                print("âœ“ å¯¹è¯å·²ä¿å­˜åˆ°è®°å¿†")
            else:
                print("âš ï¸ æœªè·å–åˆ°AIå›å¤")

    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç¨‹åºå·²é€€å‡º")
    except Exception as e:
        print(f"âŒ ç¨‹åºå¼‚å¸¸é€€å‡º: {e}")
