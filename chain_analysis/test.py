import asyncio
import sys
from langchain.memory import ConversationBufferWindowMemory
from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.prebuilt import create_react_agent
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from pydantic import SecretStr

# åˆå§‹åŒ– ChatOpenAI å¤§æ¨¡å‹å®¢æˆ·ç«¯
# llm = ChatOpenAI(
#     model="ep-20*********02-887jj",  # ç«å±±å¼•æ“æ¨ç†æ¥å…¥ç‚¹
#     api_key=SecretStr("b6a98d76-**********-b490f6ba59e3"),  # ç«å±±å¼•æ“APIå¯†é’¥
#     base_url="https://ark.cn-beijing.volces.com/api/v3",  # ç«å±±å¼•æ“APIåŸºç¡€åœ°å€
#     temperature=0.3  # é™ä½æ¸©åº¦å€¼ï¼Œå‡å°‘éšæœºæ€§
# )
llm = ChatOpenAI(
    model="qwen-max",  # æŒ‡å®šåƒé—®æ¨¡å‹
    temperature=0,
    api_key="sk-7b957493cc324362b63a95bcd09ef189",  # ç¡®ä¿ç¯å¢ƒå˜é‡æ­£ç¡®
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"  # å…¼å®¹æ¨¡å¼ç«¯ç‚¹
)


# åˆå§‹åŒ–LLM
# llm = ChatOllama(
#     model="qwen3:14b",
#     base_url="http://localhost:11434",
#     temperature=0.3
# )

# åˆ›å»ºä¼˜åŒ–çš„æç¤ºè¯
def create_smart_prompt():
    """åˆ›å»ºæ™ºèƒ½æç¤ºè¯ï¼Œå¼•å¯¼AIåˆç†ä½¿ç”¨å·¥å…·"""
    system_template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ™ºèƒ½åŠ©æ‰‹ï¼Œå…·å¤‡å¤šç§å·¥å…·èƒ½åŠ›ï¼Œèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·å®Œæˆå„ç±»ä»»åŠ¡ã€‚

å¯ç”¨å·¥å…·åŠä½¿ç”¨åœºæ™¯ï¼š
- terminal: æ‰§è¡Œç³»ç»Ÿå‘½ä»¤ã€æ–‡ä»¶æ“ä½œã€ç¨‹åºè¿è¡Œ
- app_analysis: èŒä¸šåˆ†æã€åº”ç”¨ç¨‹åºåˆ†æã€ç”¨æˆ·ç”»åƒ  
- browser: ç½‘é¡µæ§åˆ¶ã€è‡ªåŠ¨åŒ–æ“ä½œã€ä¿¡æ¯é‡‡é›†
- weather: å¤©æ°”æŸ¥è¯¢ã€æ°”è±¡ä¿¡æ¯è·å–

å·¥å…·ä½¿ç”¨åŸåˆ™ï¼š
1. åªæœ‰ç”¨æˆ·æ˜ç¡®éœ€è¦æ‰§è¡Œå…·ä½“æ“ä½œæ—¶æ‰ä½¿ç”¨å·¥å…·
2. å¯¹äºä¸€èˆ¬æ€§å¯¹è¯ã€æ¦‚å¿µè§£é‡Šã€å»ºè®®å’¨è¯¢ç­‰ï¼Œç›´æ¥å›ç­”
3. ä¼˜å…ˆé€‰æ‹©æœ€åˆé€‚çš„å·¥å…·æ¥å®Œæˆä»»åŠ¡
4. å¦‚æœä¸ç¡®å®šæ˜¯å¦éœ€è¦å·¥å…·ï¼Œä¼˜å…ˆé€‰æ‹©ç›´æ¥å›ç­”

è¯·ä¿æŒè‡ªç„¶ã€å‹å¥½çš„å¯¹è¯é£æ ¼ï¼Œå‡†ç¡®ç†è§£ç”¨æˆ·çœŸå®æ„å›¾ï¼Œæ˜æ™ºåœ°é€‰æ‹©æ˜¯å¦ä½¿ç”¨å·¥å…·ã€‚"""

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_template),
        MessagesPlaceholder(variable_name="messages")
    ])

    return prompt


# ç®€åŒ–çš„ç»“æœè¾“å‡ºå‡½æ•°
def print_result(agent_response):
    """è¾“å‡ºä»£ç†å“åº”ç»“æœ"""
    messages = agent_response.get("messages", [])

    # æå–æœ€ç»ˆç­”æ¡ˆ
    for message in reversed(messages):
        if message.type == "ai" and message.content:
            print(f"\nğŸ¯ å›ç­”: {message.content}")
            break


# ä¸»å‡½æ•°
async def main():
    """
    ä¸»å‡½æ•° - é›†æˆLangChainè®°å¿†åŠŸèƒ½çš„å¯¹è¯å®¢æˆ·ç«¯

    Author: FangGL
    Date: 2024-12-19
    """
    client = None

    # åˆå§‹åŒ–MCPå®¢æˆ·ç«¯ - å®Œæ•´æœåŠ¡å™¨é…ç½®
    client = MultiServerMCPClient({  # type: ignore
        "terminal": {
            "command": "python",
            "args": ["D://OrdinaryYQ/code/python/llm-agent/chain_analysis/mcp_servers/terminal_server.py"],
            "transport": "stdio",
        },
        "app_analysis": {
            "command": "python",
            "args": ["D://OrdinaryYQ/code/python/llm-agent/chain_analysis/mcp_servers/application_analysis_server.py"],
            "transport": "stdio",
        },
        "browser": {
            "command": "python",
            "args": ["D://OrdinaryYQ/code/python/llm-agent/chain_analysis/mcp_servers/browser_control_server.py"],
            "transport": "stdio",
        },
        "weather": {
            "command": "python",
            "args": ["D://OrdinaryYQ/code/python/llm-agent/chain_analysis/mcp_servers/weather_server.py"],
            "transport": "stdio",
        }
    })

    tools = await client.get_tools()
    print(tools)



if __name__ == "__main__":
    asyncio.run(main())
